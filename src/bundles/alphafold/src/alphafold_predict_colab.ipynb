{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alphafold_predict_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "nIH5LqSxZbdK"
      },
      "source": [
        "# ================================================================================================\n",
        "# Google Colab code for running an AlphaFold structure prediction.\n",
        "#\n",
        "\n",
        "# Make sure virtual machine has a GPU\n",
        "def check_for_gpu():\n",
        "    import jax\n",
        "    devtype = jax.local_devices()[0].platform\n",
        "    if devtype == 'gpu':\n",
        "        print ('Have Colab GPU runtime')\n",
        "    else:\n",
        "        raise RuntimeError('Require Colab GPU runtime, got %s.\\n' % devtype +\n",
        "                           'Change GPU with Colab menu\\n' +\n",
        "                           'Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "\n",
        "def is_alphafold_installed():\n",
        "    try:\n",
        "        import alphafold\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def install_alphafold(ALPHAFOLD_GIT_REPO = 'https://github.com/deepmind/alphafold',\n",
        "                      ALPHAFOLD_PARAMETERS = 'https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar',\n",
        "                      BOND_PARAMETERS = 'https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt',\n",
        "                      INSTALL_LOG = 'install_log.txt'):\n",
        "\n",
        "    PARAMS_DIR = './alphafold/data/params'\n",
        "    import os.path\n",
        "    PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(ALPHAFOLD_PARAMETERS))\n",
        "\n",
        "    cmds = f'''\n",
        "# Get AlphaFold from GitHub and install it\n",
        "git clone {ALPHAFOLD_GIT_REPO} alphafold\n",
        "pip3 install ./alphafold\n",
        "\n",
        "# Get AlphaFold parameters, 3.5 Gbytes,\n",
        "# Ten models model_1, model_2, ..., model_5, model_1_ptm, ..., model_5_ptm.\n",
        "mkdir -p \"{PARAMS_DIR}\"\n",
        "wget -q -O \"{PARAMS_PATH}\" {ALPHAFOLD_PARAMETERS}\n",
        "tar --extract --verbose --file=\"{PARAMS_PATH}\" --directory=\"{PARAMS_DIR}\" --preserve-permissions\n",
        "rm \"{PARAMS_PATH}\"\n",
        "\n",
        "# Get standard bond length and bond angle parameters\n",
        "mkdir -p /content/alphafold/common\n",
        "wget -q -P /content/alphafold/common {BOND_PARAMETERS}\n",
        "\n",
        "# Create a ramdisk to store a database chunk to make jackhmmer run fast.\n",
        "# Module alphafold.data.tools.jackhmmer makes use of this /tmp/ramdisk.\n",
        "sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "'''\n",
        "    run_shell_commands(cmds, 'install_alphafold.sh', INSTALL_LOG)\n",
        "    \n",
        "def run_shell_commands(commands, filename, INSTALL_LOG):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(commands)\n",
        "\n",
        "    # The -x option logs each command with a prompt in front of it.\n",
        "    !bash -x \"{filename}\" >> \"{INSTALL_LOG}\" 2>&1\n",
        "    \n",
        "def install_hmmer(INSTALL_LOG = 'install_log.txt'):\n",
        "    # Install HMMER package in /usr/bin\n",
        "    cmds = '''sudo apt install --quiet --yes hmmer'''\n",
        "    run_shell_commands(cmds, 'install_hmmer.sh', INSTALL_LOG)\n",
        "    \n",
        "def install_openmm(CONDA_INSTALL_SH = 'https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh',\n",
        "                   INSTALL_LOG = 'install_log.txt'):\n",
        "    '''Must install alphafold first since an openmm patch from alphafold is used.'''\n",
        "    # Install Conda\n",
        "    import os.path\n",
        "    CONDA_INSTALL = os.path.join('/tmp', os.path.basename(CONDA_INSTALL_SH))\n",
        "    cmds = f'''\n",
        "wget -q -P /tmp {CONDA_INSTALL_SH} \\\n",
        "    && bash \"{CONDA_INSTALL}\" -b -p /opt/conda \\\n",
        "    && rm \"{CONDA_INSTALL}\"\n",
        "\n",
        "# Install Python, OpenMM and pdbfixer in Conda\n",
        "/opt/conda/bin/conda update -qy conda && \\\n",
        "    /opt/conda/bin/conda install -qy -c conda-forge python=3.7 openmm=7.5.1 pdbfixer\n",
        "\n",
        "\n",
        "# Patch OpenMM\n",
        "(cd /opt/conda/lib/python3.7/site-packages/ && \\\n",
        "    patch -p0 < /content/alphafold/docker/openmm.patch)\n",
        "\n",
        "# Put OpenMM and pdbfixer in ipython path which includes current directory /content\n",
        "ln -s /opt/conda/lib/python3.7/site-packages/simtk .\n",
        "ln -s /opt/conda/lib/python3.7/site-packages/pdbfixer .\n",
        "'''\n",
        "    run_shell_commands(cmds, 'install_openmm.sh', INSTALL_LOG)\n",
        "\n",
        "# ================================================================================================\n",
        "# Python routines to run a prediction.\n",
        "#\n",
        "\n",
        "# Check sequence\n",
        "def check_sequence(sequence, MIN_SEQUENCE_LENGTH = 16, MAX_SEQUENCE_LENGTH = 2500):\n",
        "    # Remove all whitespaces, tabs and end lines; upper-case\n",
        "    sequence = sequence.translate(str.maketrans('', '', ' \\n\\t')).upper()\n",
        "    aatypes = set('ACDEFGHIKLMNPQRSTVWY')  # 20 standard aatypes\n",
        "    if not set(sequence).issubset(aatypes):\n",
        "        raise Exception(f'Input sequence contains non-amino acid letters: {set(sequence) - aatypes}. AlphaFold only supports 20 standard amino acids as inputs.')\n",
        "    if len(sequence) < MIN_SEQUENCE_LENGTH:\n",
        "        raise Exception(f'Input sequence is too short: {len(sequence)} amino acids, while the minimum is {MIN_SEQUENCE_LENGTH}')\n",
        "    if len(sequence) > MAX_SEQUENCE_LENGTH:\n",
        "        raise Exception(f'Input sequence is too long: {len(sequence)} amino acids, while the maximum is {MAX_SEQUENCE_LENGTH}. Please use the full AlphaFold system for long sequences.')\n",
        "    return sequence\n",
        "\n",
        "# Make table of sequence databases to be searched\n",
        "def sequence_databases():\n",
        "    mirror = fastest_sequence_db_mirror()\n",
        "    db_prefix = f'https://storage.googleapis.com/alphafold-colab{mirror}/latest'\n",
        "    databases = [\n",
        "        {\n",
        "            'name': 'uniref90',\n",
        "            'url': db_prefix + '/uniref90_2021_03.fasta',\n",
        "            'num chunks':59,\n",
        "            'max hits': 10000,\n",
        "            'z value': 135301051\n",
        "        },\n",
        "        {\n",
        "            'name': 'smallbfd',\n",
        "            'url': db_prefix + '/bfd-first_non_consensus_sequences.fasta',\n",
        "            'num chunks': 17,\n",
        "            'max hits': 10000,\n",
        "            'z value': 65984053,\n",
        "        },\n",
        "        {\n",
        "            'name': 'mgnify',\n",
        "            'url': db_prefix + '/mgy_clusters_2019_05.fasta',\n",
        "            'num chunks': 71,\n",
        "            'max hits': 500,\n",
        "            'z value': 304820129,\n",
        "        },\n",
        "    ]\n",
        "    if fast_test:\n",
        "        databases = [db for db in databases if db['name'] == 'smallbfd']\n",
        "        databases[0]['num chunks'] = 5\n",
        "    return databases\n",
        "\n",
        "# Find the fastest responding mirror for sequence databases\n",
        "def fastest_sequence_db_mirror(test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2021_03.fasta.1'):\n",
        "    print ('Finding fastest mirror for sequence databases', end = '')\n",
        "    from concurrent import futures\n",
        "    ex = futures.ThreadPoolExecutor(3)\n",
        "    def fetch(source):\n",
        "        from urllib import request\n",
        "        request.urlretrieve(test_url_pattern.format(source))\n",
        "        return source\n",
        "    fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
        "    source = None\n",
        "    for f in futures.as_completed(fs):\n",
        "      source = f.result()\n",
        "      ex.shutdown()\n",
        "      break\n",
        "    mirror = (source[1:] if source else 'united states')\n",
        "    print (' using', mirror)\n",
        "    return source\n",
        "\n",
        "# Search against 1 Gbyte chunks of sequence databases streamed from the web.\n",
        "def jackhmmer_sequence_search(seq_file, databases, jackhmmer_binary_path = '/usr/bin/jackhmmer'):\n",
        "\n",
        "    dbs = []\n",
        "    for db in databases:\n",
        "        db_name = db['name']\n",
        "        nchunks = db['num chunks']\n",
        "        print ('Searching %s sequence database, %d Gbytes' % (db_name, nchunks))\n",
        "        def progress_cb(i):\n",
        "            print (' %d' % i, end = ('\\n' if i%30 == 0 else ''), flush = True)\n",
        "\n",
        "        from alphafold.data.tools import jackhmmer\n",
        "        jackhmmer_runner = jackhmmer.Jackhmmer(\n",
        "            binary_path=jackhmmer_binary_path,\n",
        "            database_path=db['url'],\n",
        "            get_tblout=True,\n",
        "            num_streamed_chunks=db['num chunks'],\n",
        "            streaming_callback = progress_cb,\n",
        "            z_value=db['z value'])\n",
        "        dbs.append((db_name, jackhmmer_runner.query(seq_file), db['max hits']))\n",
        "        print ('')\n",
        "\n",
        "    return dbs\n",
        "\n",
        "# Extract the multiple sequence alignments from the Stockholm files.\n",
        "def multiple_seq_align(dbs):\n",
        "    msas = []\n",
        "    deletion_matrices = []\n",
        "    seen_already = set()\n",
        "    db_counts = []\n",
        "    for db_name, db_results, max_hits in dbs:\n",
        "      unsorted_results = []\n",
        "      for i, result in enumerate(db_results):\n",
        "        from alphafold.data import parsers\n",
        "        msa, deletion_matrix, target_names = parsers.parse_stockholm(result['sto'])\n",
        "        e_values_dict = parsers.parse_e_values_from_tblout(result['tbl'])\n",
        "        e_values = [e_values_dict[t.split('/')[0]] for t in target_names]\n",
        "        zipped_results = zip(msa, deletion_matrix, target_names, e_values)\n",
        "        if i != 0:\n",
        "          # Only take query from the first chunk\n",
        "          zipped_results = [x for x in zipped_results if x[2] != 'query']\n",
        "        unsorted_results.extend(zipped_results)\n",
        "      sorted_by_evalue = sorted(unsorted_results, key=lambda x: x[3])\n",
        "      db_msas, db_deletion_matrices, _, _ = zip(*sorted_by_evalue)\n",
        "\n",
        "      # Remove duplicates\n",
        "      db_msas_uniq = []\n",
        "      db_deletion_matrices_uniq = []\n",
        "      for msa, dmat in zip(db_msas, db_deletion_matrices):\n",
        "          if msa not in seen_already:\n",
        "              seen_already.add(msa)\n",
        "              db_msas_uniq.append(msa)\n",
        "              db_deletion_matrices_uniq.append(dmat)\n",
        "      db_msas, db_deletion_matrices = db_msas_uniq, db_deletion_matrices_uniq\n",
        "\n",
        "      if db_msas:\n",
        "        if max_hits is not None:\n",
        "          db_msas = db_msas[:max_hits]\n",
        "          db_deletion_matrices = db_deletion_matrices[:max_hits]\n",
        "        msas.append(db_msas)\n",
        "        deletion_matrices.append(db_deletion_matrices)\n",
        "        db_counts.append((db_name, len(db_msas)))\n",
        "\n",
        "    total = sum([count for name, count in db_counts], 0)\n",
        "    counts = ', '.join('%d %s' % (count,name) for name, count in db_counts)\n",
        "    print('%d similar sequences found (%s)' % (total, counts))\n",
        "    return msas, deletion_matrices\n",
        "\n",
        "# Predict the structures\n",
        "def predict_structure(sequence, msas, deletion_matrices, model_names):\n",
        "    plddts = {}\n",
        "    pae_outputs = {}\n",
        "    unrelaxed_proteins = {}\n",
        "\n",
        "    num_templates = 0\n",
        "    num_res = len(sequence)\n",
        "    print('Computing structures using %d AlphaFold parameter sets:' % len(model_names))\n",
        "    for model_name in model_names:\n",
        "        print(' ' + model_name, end = '', flush = True)\n",
        "        feature_dict = {}\n",
        "        from alphafold.data import pipeline\n",
        "        feature_dict.update(pipeline.make_sequence_features(sequence, 'test', num_res))\n",
        "        feature_dict.update(pipeline.make_msa_features(msas, deletion_matrices=deletion_matrices))\n",
        "        feature_dict.update(_placeholder_template_feats(num_templates, num_res))\n",
        "\n",
        "        from alphafold.model import config, data, model\n",
        "        cfg = config.model_config(model_name)\n",
        "        params = data.get_model_haiku_params(model_name, './alphafold/data')\n",
        "        model_runner = model.RunModel(cfg, params)\n",
        "        processed_feature_dict = model_runner.process_features(feature_dict, random_seed=0)\n",
        "        prediction_result = model_runner.predict(processed_feature_dict)\n",
        "\n",
        "        if 'predicted_aligned_error' in prediction_result:\n",
        "            pae_outputs[model_name] = (\n",
        "                prediction_result['predicted_aligned_error'],\n",
        "                prediction_result['max_predicted_aligned_error']\n",
        "            )\n",
        "        plddts[model_name] = prediction_result['plddt']\n",
        "\n",
        "        # Set the b-factors to the per-residue plddt.\n",
        "        final_atom_mask = prediction_result['structure_module']['final_atom_mask']\n",
        "        b_factors = prediction_result['plddt'][:, None] * final_atom_mask\n",
        "        from alphafold.common import protein\n",
        "        unrelaxed_protein = protein.from_prediction(processed_feature_dict,\n",
        "                                                    prediction_result,\n",
        "                                                    b_factors=b_factors)\n",
        "        unrelaxed_proteins[model_name] = unrelaxed_protein\n",
        "\n",
        "        # Delete unused outputs to save memory.\n",
        "        del model_runner\n",
        "        del params\n",
        "        del prediction_result\n",
        "    print('')\n",
        "    \n",
        "    return unrelaxed_proteins, plddts, pae_outputs\n",
        "\n",
        "def write_unrelaxed_pdbs(unrelaxed_proteins, pae_outputs, output_dir):\n",
        "    # Write out PDB files and predicted alignment error\n",
        "    from alphafold.common import protein\n",
        "    for model_name, unrelaxed_protein in unrelaxed_proteins.items():\n",
        "        write_pdb(protein.to_pdb(unrelaxed_protein), model_name + '_unrelaxed.pdb', output_dir)\n",
        "        # Save predicted aligned error (if it exists)\n",
        "        if model_name in pae_outputs:\n",
        "            import os.path\n",
        "            pae_output_path = os.path.join(output_dir, model_name + '_pae.json')\n",
        "            save_predicted_aligned_error(pae_outputs[model_name], pae_output_path)\n",
        "\n",
        "def write_best_pdb(plddts, unrelaxed_proteins, output_dir):\n",
        "    # Find the best model according to the mean pLDDT.\n",
        "    best_model_name = max(plddts.keys(), key=lambda x: plddts[x].mean())\n",
        "\n",
        "    # AMBER relax the best model\n",
        "    print('Energy minimizing best structure with OpenMM')\n",
        "    relaxed_pdb = energy_minimize_structure(unrelaxed_proteins[best_model_name])\n",
        "\n",
        "    # Write out the prediction\n",
        "    write_pdb(relaxed_pdb, best_model_name + '_relaxed.pdb', output_dir)\n",
        "    write_pdb(relaxed_pdb, 'best_model.pdb', output_dir)\n",
        "\n",
        "def energy_minimize_structure(pdb_model):\n",
        "    from alphafold.relax import relax\n",
        "    amber_relaxer = relax.AmberRelaxation(\n",
        "        max_iterations=0,\n",
        "        tolerance=2.39,\n",
        "        stiffness=10.0,\n",
        "        exclude_residues=[],\n",
        "        max_outer_iterations=20)\n",
        "    relaxed_pdb, _, _ = amber_relaxer.process(prot=pdb_model)\n",
        "    return relaxed_pdb\n",
        "\n",
        "def write_pdb(pdb_model, filename, output_dir):\n",
        "    import os.path\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    with open(output_path, 'w') as f:\n",
        "      f.write(pdb_model)\n",
        "\n",
        "def _placeholder_template_feats(num_templates_, num_res_):\n",
        "  from numpy import zeros, float32\n",
        "  return {\n",
        "      'template_aatype': zeros([num_templates_, num_res_, 22], float32),\n",
        "      'template_all_atom_masks': zeros([num_templates_, num_res_, 37, 3], float32),\n",
        "      'template_all_atom_positions': zeros([num_templates_, num_res_, 37], float32),\n",
        "      'template_domain_names': zeros([num_templates_], float32),\n",
        "      'template_sum_probs': zeros([num_templates_], float32),\n",
        "  }\n",
        "\n",
        "def save_predicted_aligned_error(model_pae, pae_output_path):\n",
        "  # Save predicted aligned error in the same format as the AF EMBL DB\n",
        "  pae, max_pae = model_pae\n",
        "  import numpy as np\n",
        "  rounded_errors = np.round(pae.astype(np.float64), decimals=1)\n",
        "  indices = np.indices((len(rounded_errors), len(rounded_errors))) + 1\n",
        "  indices_1 = indices[0].flatten().tolist()\n",
        "  indices_2 = indices[1].flatten().tolist()\n",
        "  pae_data = [{\n",
        "      'residue1': indices_1,\n",
        "      'residue2': indices_2,\n",
        "      'distance': rounded_errors.flatten().tolist(),\n",
        "      'max_predicted_aligned_error': max_pae.item()\n",
        "  }]\n",
        "  import json\n",
        "  json_data = json.dumps(pae_data, indent=None, separators=(',', ':'))\n",
        "  with open(pae_output_path, 'w') as f:\n",
        "    f.write(json_data)\n",
        "\n",
        "def predict_and_save(sequence, databases, output_dir = 'prediction',\n",
        "                     model_names = ['model_1', 'model_2', 'model_3', 'model_4', 'model_5']):\n",
        "    '''\n",
        "    Model names refer to alphafold parameter sets.\n",
        "    One structure is calculated for each model name.\n",
        "    Model names ending in \"_ptm\" predict TM score ('model_1_ptm', ..., 'model_5_ptm').\n",
        "    '''\n",
        "\n",
        "    # Write target sequence to file in FASTA format for doing search.\n",
        "    seq_file = 'target.fasta'\n",
        "    with open(seq_file, 'wt') as f:\n",
        "      f.write(f'>query\\n{sequence}')\n",
        "\n",
        "    # Search for sequences\n",
        "    nchunks = sum(db['num chunks'] for db in databases)\n",
        "    print ('Searching sequence databases (%d Gbytes).' % nchunks)\n",
        "    print ('Search will take %d minutes or more.' % max(1,nchunks//5))\n",
        "    dbs = jackhmmer_sequence_search(seq_file, databases)\n",
        "\n",
        "    # Make multiple sequence alignment.\n",
        "    print ('Computing multiple sequence alignment')\n",
        "    msas, deletion_matrices = multiple_seq_align(dbs)\n",
        "\n",
        "    # Create directory for writing structure files\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Predict structures\n",
        "    unrelaxed_proteins, plddts, pae_outputs = \\\n",
        "        predict_structure(sequence, msas, deletion_matrices, model_names)\n",
        "\n",
        "    # Write out PDB files and predicted errors\n",
        "    write_unrelaxed_pdbs(unrelaxed_proteins, pae_outputs, output_dir)\n",
        "    write_best_pdb(plddts, unrelaxed_proteins, output_dir)\n",
        "\n",
        "    print ('Structure prediction completed.')\n",
        "    \n",
        "def set_environment_variables():\n",
        "    # Set memory management environment variables used by AlphaFold dependencies\n",
        "    import os\n",
        "    os.environ['TF_FORCE_UNIFIED_MEMORY'] = '1'\n",
        "    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '2.0'\n",
        "\n",
        "def run_prediction(sequence, output_dir = 'prediction', INSTALL_LOG = 'install_log.txt'):\n",
        "    '''Installs alphafold if not yet installed and runs a stucture prediction.'''\n",
        "\n",
        "    # Check sequence length are within limits and no illegal characters\n",
        "    sequence = check_sequence(sequence)\n",
        "    print ('Sequence length %d' % len(sequence))\n",
        "    \n",
        "    # Check for GPU at beginning.\n",
        "    # If no GPU then enabling a GPU runtime clears all virtual machine state\n",
        "    # so need to enable GPU runtime before installing the prerequisites.\n",
        "    check_for_gpu()\n",
        "\n",
        "    if not is_alphafold_installed():\n",
        "        print ('Installing HMMER for computing sequence alignments')\n",
        "        install_hmmer(INSTALL_LOG = INSTALL_LOG)\n",
        "        print ('Installing AlphaFold')\n",
        "        install_alphafold(INSTALL_LOG = INSTALL_LOG)\n",
        "        print ('Installing OpenMM for structure energy minimization')\n",
        "        install_openmm(INSTALL_LOG = INSTALL_LOG)\n",
        "\n",
        "    set_environment_variables()\n",
        "    databases = sequence_databases()\n",
        "\n",
        "    if fast_test:\n",
        "        predict_and_save(sequence, databases, output_dir, model_names = ['model_1'])\n",
        "    else:\n",
        "        predict_and_save(sequence, databases, output_dir)\n",
        "\n",
        "    # Make a zip file of the predictions\n",
        "#    !zip -q -r {output_dir}.zip {output_dir}\n",
        "\n",
        "    # Download predictions.  Does not work on Safari 14.1, macOS 10.15.7\n",
        "    from google.colab import files\n",
        "    files.download(f'{output_dir}/best_model.pdb')\n",
        "#    files.download(f'{output_dir}.zip')\n",
        "\n",
        "# ================================================================================================\n",
        "# Predict a structure for a sequence.\n",
        "#\n",
        "fast_test = False\n",
        "sequence = 'Paste a sequence here'  #@param {type:\"string\"}\n",
        "#sequence = 'QVQLVESGGGSVQAGGSLRLSCTASGGSEYSYSTFSLGWFRQAPGQEREAVAAIASMGGLTYYADSVKGRFTISRDNAKNTVTLQMNNLKPEDTAIYYCAAVRGYFMRLPSSHNFRYWGQGTQVTVSSRGR'  #@param {type:\"string\"}\n",
        "\n",
        "run_prediction(sequence)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}